import csv
import os
import re
from google.cloud import storage, bigquery

# ç’°å¢ƒå¤‰æ•°ã§æŒ‡å®šã—ã¦ã‚‚ã‚ˆã„
PROJECT = "opproject-459908"
DATASET = "SNS"
TABLE = "MeltWater"


# --- CSVã®åˆ—æ•°ã‚’æƒãˆã‚‹é–¢æ•°ï¼ˆç©ºè¡Œã‚¹ã‚­ãƒƒãƒ—ä»˜ãï¼‰ ---
def ensure_fixed_column_count(input_path, output_path, expected_cols=42):
    with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8', newline='') as outfile:
        reader = csv.reader(infile)
        writer = csv.writer(outfile)
        for row in reader:
            # âœ… ç©ºè¡Œã¾ãŸã¯ã™ã¹ã¦ç©ºã®è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
            if not row or all(cell.strip() == '' for cell in row):
                continue
            # åˆ—æ•°ã‚’èª¿æ•´
            if len(row) < expected_cols:
                row += [''] * (expected_cols - len(row))
            elif len(row) > expected_cols:
                row = row[:expected_cols]
            writer.writerow(row)

# --- ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆCloud Functionï¼‰ ---
def main(event, context):
    file_data = event
    file_name = file_data['name']
    bucket_name = file_data['bucket']

    if not file_name.endswith('.csv'):
        print(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {file_name} ã¯CSVã§ã¯ã‚ã‚Šã¾ã›ã‚“")
        return

    # âœ… fixed/ ãƒ•ã‚©ãƒ«ãƒ€ã«å«ã¾ã‚Œã¦ã„ãŸã‚‰å‡¦ç†ã—ãªã„ï¼ˆå†å¸°é˜²æ­¢ï¼‰
    if re.search(r'(^|/)fixed(/|$)', file_name):
        print(f"â­ï¸ fixed ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç„¡è¦–ã—ã¾ã™: {file_name}")
        return

    print(f"ğŸ“¥ GCSã‹ã‚‰CSVã‚’å–å¾—: {bucket_name}/{file_name}")

    # GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(file_name)

    # ä¸€æ™‚ä¿å­˜ãƒ‘ã‚¹
    local_path = f"/tmp/{os.path.basename(file_name)}"
    fixed_path = f"/tmp/fixed_{os.path.basename(file_name)}"

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    blob.download_to_filename(local_path)

    # åˆ—æ•°ã‚’æƒãˆã‚‹
    ensure_fixed_column_count(local_path, fixed_path, expected_cols=42)

    # æ•´å½¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ¥GCSãƒ‘ã‚¹ã¸å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    fixed_blob = bucket.blob(f"fixed/{os.path.basename(file_name)}")
    fixed_blob.upload_from_filename(fixed_path)

    # BigQueryã«ãƒ­ãƒ¼ãƒ‰
    upload_to_bigquery(f"gs://{bucket_name}/fixed/{os.path.basename(file_name)}")

# --- BigQueryã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‡¦ç† ---
def upload_to_bigquery(gcs_uri):
    client = bigquery.Client()
    table_id = f"{PROJECT}.{DATASET}.{TABLE}"

    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.CSV,
        skip_leading_rows=1,
        autodetect=True,
        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
        encoding="UTF-8",
    )

    try:
        load_job = client.load_table_from_uri(gcs_uri, table_id, job_config=job_config)
        print(f"â–¶ï¸ BigQuery LoadJob started: {load_job.job_id}")
        load_job.result()  # âœ… ã“ã“ã§å¤±æ•—ã™ã‚‹ã¨ä¾‹å¤–ã«ãªã‚‹
        print(f"âœ… BigQuery å–è¾¼æˆåŠŸ: {table_id}")
    except Exception as e:
        print(f"âŒ BigQuery å–è¾¼å¤±æ•—: {e}")
        raise  # ã‚¨ãƒ©ãƒ¼ã‚’Cloud Functionsã®ãƒ­ã‚°ã«æ®‹ã™
