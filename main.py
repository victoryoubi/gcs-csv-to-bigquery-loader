import csv
import os
from google.cloud import storage, bigquery

# ç’°å¢ƒå¤‰æ•°ã§æŒ‡å®šã—ã¦ã‚‚ã‚ˆã„
PROJECT = "opproject-459908"
DATASET = "SNS"
TABLE = "MeltWater"

# --- CSVã®åˆ—æ•°ã‚’æƒãˆã‚‹é–¢æ•° ---
def ensure_fixed_column_count(input_path, output_path, expected_cols=42):
    with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'w', encoding='utf-8', newline='') as outfile:
        reader = csv.reader(infile)
        writer = csv.writer(outfile)
        for row in reader:
            if len(row) < expected_cols:
                row += [''] * (expected_cols - len(row))
            elif len(row) > expected_cols:
                row = row[:expected_cols]
            writer.writerow(row)

# --- ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆCloud Functionï¼‰ ---
def main(event, context):
    file_data = event
    file_name = file_data['name']
    bucket_name = file_data['bucket']

    if not file_name.endswith('.csv'):
        print(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—: {file_name} ã¯CSVã§ã¯ã‚ã‚Šã¾ã›ã‚“")
        return

    if file_name.startswith('fixed/'):
        print(f"â­ï¸ fixed ãƒ•ã‚©ãƒ«ãƒ€å†…ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ç„¡è¦–ã—ã¾ã™: {file_name}")
        return

    print(f"ğŸ“¥ GCSã‹ã‚‰CSVã‚’å–å¾—: {bucket_name}/{file_name}")

    # GCSã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
    storage_client = storage.Client()
    bucket = storage_client.bucket(bucket_name)
    blob = bucket.blob(file_name)

    # ä¸€æ™‚ä¿å­˜ãƒ‘ã‚¹
    local_path = f"/tmp/{os.path.basename(file_name)}"
    fixed_path = f"/tmp/fixed_{os.path.basename(file_name)}"

    # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
    blob.download_to_filename(local_path)

    # åˆ—æ•°ã‚’æƒãˆã‚‹
    ensure_fixed_column_count(local_path, fixed_path, expected_cols=42)

    # æ•´å½¢ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ¥GCSãƒ‘ã‚¹ã¸å†ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    fixed_blob = bucket.blob(f"fixed/{os.path.basename(file_name)}")
    fixed_blob.upload_from_filename(fixed_path)

    # BigQueryã«ãƒ­ãƒ¼ãƒ‰
    upload_to_bigquery(f"gs://{bucket_name}/fixed/{os.path.basename(file_name)}")


# --- BigQueryã«ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‡¦ç† ---
def upload_to_bigquery(gcs_uri):
    client = bigquery.Client()
    table_id = f"{PROJECT}.{DATASET}.{TABLE}"

    job_config = bigquery.LoadJobConfig(
        source_format=bigquery.SourceFormat.CSV,
        skip_leading_rows=1,
        autodetect=True,
        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
        encoding="UTF-8",
    )

    load_job = client.load_table_from_uri(gcs_uri, table_id, job_config=job_config)
    load_job.result()
    print(f"âœ… BigQueryã¸ãƒ­ãƒ¼ãƒ‰å®Œäº†: {table_id}")
